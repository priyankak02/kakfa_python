`Apache Kafka Practical part





Lab 1 : Apache Kafka and Zookeeper Installation on Linux using WSL 2

Pre-requisite : WSL2 up and running on Windows 10/11

Use the following address to access WSL files from windows \\wsl$\Ubuntu-20.04

Step 0 : Update your Operating system


sudo apt update

NOTE : If you get the error “Updates for this repository will not be applied.” Use the following command

sudo hwclock –hctosys

Step 1 : Installing Java JDK 11


sudo apt install default-jre

(or)


wget -O- https://apt.corretto.aws/corretto.key | sudo apt-key add -

sudo add-apt-repository 'deb https://apt.corretto.aws stable main'

sudo apt-get update; sudo apt-get install -y java-11-amazon-corretto-jdk



Upon completion, you should get a similar output when doing java -version:








Step 2: Download the latest version of Apache Kafka from https://kafka.apache.org/downloads under Binary downloads.

  
Alternatively use this command using wget


wget https://downloads.apache.org/kafka/3.4.1/kafka_2.13-3.4.1.tgz

Step 3: Download and extract the contents to a directory of your choice, for example ~/kafka


tar xzf kafka_2.13-3.4.1.tgz

mv kafka_2.13-3.4.1 kafka

Step 4: Open a Shell and navigate to the root directory of Apache Kafka. For this example, we will assume that the Kafka download is expanded into the ~/kafka directory

Step 5: Start Zookeeper

From the root of Apache Kafka, run the following command to start Zookeeper:

~/kafka/bin/zookeeper-server-start.sh ~/kafka/config/zookeeper.properties

Step 6: Start Apache Kafka Server

Open another Shell window and run the following command from the root of Apache Kafka to start Apache Kafka.

~/kafka/bin/kafka-server-start.sh ~/kafka/config/server.properties



NOTE : Ensure to keep both shell windows opened, otherwise you will shut down Kafka or Zookeeper.



Kafka is now started, congratulations!




Note : Setup the $PATH environment variable

In order to easily access the Kafka binaries, you can edit your PATH variable by adding the following line to your system run commands

nano ~/.bashrc

PATH="$PATH:~/kafka/bin"

This ensures that you can now run the kafka commands without prefixing them.

After reloading your shell, the following should work from any directory
For shell reloading –
Close terminal and open it again    OR
Execute command bash    OR
Execute command source ~/.bashrc



Use telnet localhost 2181 to check if Zookeeper is running or not

Type srvr to check

To check which brokers are connected to Zookeeper

zookeeper-shell.sh localhost:2181 ls /brokers/ids
 



Lab 2 : Kafka CLI – Single Producer and Consumer (Single Node Cluster)

1.	Checking Kafka Version 
kafka-topics.sh --version

2.	Creating a Kafka topic


-	Creating the Kafka topic quickstart-events when my Kafka broker is running at localhost:9092

-	Use the kafka-topics.sh CLI with the --create option


kafka-topics.sh --create --topic quickstart-events -- bootstrap-server localhost:9092



Warning: You shouldn't mix underscores and periods in topic name. To avoid issues, it is best to use either, but not both.


3.  Checking the list of topics

To list Kafka topics, we need to provide the mandatory parameters:

-	The Kafka hostname and port e.g., localhost:9092

-	Use the kafka-topics.sh CLI with the --list option

kafka-topics.sh --bootstrap-server localhost:9092 --list 4. Describing the topic

kafka-topics.sh --describe --topic quickstart-events -- bootstrap-server localhost:9092


     5. Write some event into the topic using Producer client
kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092

6.  Read the events using Consumer client

kafka-console-consumer.sh --topic quickstart-events --from-beginning -- bootstrap-server localhost:9092

7.  Deleting the topic

kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic quickstart-events 



Task : Create a text file having 3 sentences and produce messages to the topic from the file

Hint : Use < filename.txt at the end of CLI producer command


Lab  – Producing/consuming messages in key-value pair format

1.  Create a new topic

kafka-topics.sh --create --topic key-value --bootstrap-server localhost:9092

2. Write some message in form of key-value pair using Producer

kafka-console-producer.sh --topic key-value --property parse.key=true --property key.separator="," --bootstrap-server localhost:9092

3. Read messages in form of key-value pair

kafka-console-consumer.sh --topic key-value --property print.key=true --property key.separator=',' --from-beginning --bootstrap-server localhost:9092 

4. Read messages along with timestamp value

kafka-console-consumer.sh --topic key-value --property print.key=true --property key.separator=',' --property print.timestamp=true --from-beginning --bootstrap-server localhost:9092

Lab 3 – Multiple Producer and Multiple Consumer in a Kafka Topic

Task 3.1 : Create a Single Producer and Multiple Consumer

Step 1: Create a topic

kafka-topics.sh --create --topic lab3 --bootstrap-server localhost:9092

Step 2: Producer Client

kafka-console-producer.sh --topic lab3 --bootstrap-server localhost:9092

Step 3: Consumer Client

kafka-console-consumer.sh --topic lab3 --from-beginning --bootstrap-server localhost:9092

Observations:

1.	Kafka cluster stores messages even if they are already consumed by one of its consumers

2.	Same messages can be read multiple times by different consumers

3.	Multiple consumers can read Kafka topic in parallel

Task 3.2 : Create Multiple Producer and Multiple Consumer

Repeat the Task 3.1 steps on different terminals

Observations :

1.  Producers and Consumers are decoupled


Lab 4 : Installing Python Client for Apache Kafka and running a simple program on Single Node Cluster
 


Step 1: Install Python Client for Apache Kafka


pip install kafka-python


Doc : https://kafka-python.readthedocs.io/en/master/


Note : Use the following command to install pip if not available

sudo apt install python3-pip













Step 2 : git clone https://github.com/priyankak02/kakfa_python

In /kafka-python/simple-python folder

Step 3: Run the producer.py and consumer.py programs in different terminal 

Lab 5 : Running the Apache Kafka for producing messages and storing in a csv file

Run the producer and consumer program from csv_python Folder and note the observations in separate terminals


On Terminal 1,

cd kakfa_python/csv_python/

python3 producer_csv.py

On Terminal 2,

cd kakfa_python/csv_python/

python3 consumer_csv.py

Observations :

1.	CSV file named messages is created for storing data

2.	The data produced from a producer is written to a csv file after consumed from Apache Kafka Broker







Lab 6 : Running the Apache Kafka for producing messages using a Faker library and consuming them


Run the producer and consumer program from 3_Faker_Kafka Folder and note the observations in separate terminals


Install Faker library using

pip install faker

On Terminal 1,

cd kafka_python/faker_kafka/

python3 producer_faker.py

On Terminal 2,

cd kafka-python/faker_kafka/

python3 consumer_faker.py



Observations :

1.	Faker produces data with following keys:

a.	Name

b.	Address

c.	Year

2.	The data produced from a producer is shown on the consumer file

Lab 7 : Orders

Run 4 files from Orders_Kafka Folder

•	order_details.py ( Producer )

•	transaction.py(Consumer + Producer)

•	analytics.py (Consumer)

•	email.py(Consumer)
  


Lab 8 : Partitions and Offsets in Apache Kafka

Step 1: Create a topic

kafka-topics.sh --create --topic lab8 --bootstrap-server localhost:9092 replication-factor 1 --partitions 3

If you check in tmp/kafka-logs, you will find 3 folders










Step 2: Producer Client

kafka-console-producer.sh --topic lab8 --bootstrap-server localhost:9092

Step 3: Consumer Client

kafka-console-consumer.sh --topic lab8 --from-beginning --bootstrap-server localhost:9092

Step 4: To check values from specific partition

kafka-console-consumer.sh --topic lab8 --from-beginning --bootstrap-server localhost:9092 --partition 0

Also check the logs for understanding the values of every partition Try the following : Give offset with from beginning

kafka-console-consumer.sh --topic lab8 --from-beginning --bootstrap-server localhost:9092 --partition 0 –-offset 2



Logically, you should use, using –from-beginning and offset together doesn’t make sense 

kafka-console-consumer.sh --topic lab8 --bootstrap-server localhost:9092 -- partition 0 –-offset 2

Try giving offset without partition

kafka-console-consumer.sh --topic lab8 --from-beginning --bootstrap-server localhost:9092 --offset 0

You get the following error:

The partition is required when offset is specified.


Note : Checking the offsets count

kafka-run-class.sh kafka.tools.DumpLogSegments -files /tmp/kafka-logs/lab8-2/00000000000000000000.log --print-data-log

Specify batch-size parameter

kafka-console-producer.sh --topic lab8 --bootstrap-server localhost:9092 --batch-size 1

Observation : messages will be distributed among partitions.

Lab 9 : Partitions and Offsets in Apache Kafka (Python Programming for Producer)

For creating partitions, use KafkaAdminClient API or else create topic using CLI.

Get Producer python code from given folder 


cd kafka_python/partition_producer_kafka/

1.	Partitioning using partition argument

Create a topic named test1 with 3 partitions

kafka-topics.sh --create --topic test1 --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3

Run producer1.py program in one terminal

Another terminal execute command to consume messages

kafka-console-consumer.sh --topic test1 --from-beginning --bootstrap-server localhost:9092 --partition 1

Note : The json.dumps() method encodes any Python object into JSON formatted String.
	For consuming data, python code can also be written


2.	Partitioning using key

Create a topic named test2 with 3 partitions

kafka-topics.sh --create --topic test2 --bootstrap-server localhost:9092 --replication-factor
1 --partitions 3


Run producer2.py program in one terminal

Another terminal execute command to consume messages
kafka-console-consumer.sh --topic test2 --from-beginning --bootstrap-server localhost:9092 --property print.key=true  --property print.value=true


Observation : Values with same key is guaranteed to be in same partition, however multiple values of different keys can be in the same partition

 

3.	Partitioning using key with Key and Value Serializer 

Create a topic named test3 with 3 partitions

kafka-topics.sh --create --topic test3 --bootstrap-server localhost:9092 --replication-factor
1 --partitions 3

Run producer3.py program in one terminal

Another terminal execute command to consume messages

kafka-console-consumer.sh --topic test3 --from-beginning --bootstrap-server localhost:9092 --property print.key=true  --property print.value=true --property print.partition=true

4.	Custom Partitioner

Create a topic named test4 with 3 partitions

kafka-topics.sh --create --topic test4 --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3

Run producer4.py program in one terminal

Another terminal execute command to consume messages

kafka-console-consumer.sh --topic test4 --from-beginning --bootstrap-server localhost:9092 --property print.key=true  --property print.value=true --property print.partition=true


Lab 10 : Multi-node Cluster without Replication Factor


Deleting all logs from Broker and Zookeeper

sudo rm -rf /tmp/kafka-logs /tmp/zookeeper

Releasing a specific port

sudo fuser -k 9092/tcp

To check which brokers are connected to Zookeeper

zookeeper-shell.sh localhost:2181 ls /brokers/ids

Step 1: Create 3 folders for storing the logs(data) generated by 3 brokers in temp folder named kafka_logs, kafka1_logs, kafka2_logs

Step 2: Create 3 brokers named server, server1 and server2

Update the following details in 3 properties file named server.properties, server1.proerties, server2.properties in the config file

•	Broker ids = 0,1,2

•	Port number using LISTENERS = 9092,9093,9094

•	Logs file directory in tmp folder = kafka_logs, kafka1_logs, kafka2_logs
 
With ZooKeeper on, do the following



Step 2: Start all the 3 kafka servers in 3 separate terminals

~/kafka/bin/kafka-server-start.sh ~/kafka/config/server.properties ~/kafka/bin/kafka-server-start.sh ~/kafka/config/server1.properties ~/kafka/bin/kafka-server-start.sh ~/kafka/config/server2.properties

Step 3: Create a topic named demo_testing with 5 partitions

This topic will be created in multiple partitions across 3 brokers

kafka-topics.sh --create --topic demo_testing --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --replication-factor 1 -- partitions 5

Step 4: Start a producer

kafka-console-producer.sh --topic demo_testing --bootstrap-server localhost:9092, localhost:9093, localhost:9094



Step 5: Start a consumer

kafka-console-consumer.sh --topic demo_testing --from-beginning -- bootstrap-server localhost:9092, localhost:9093, localhost:9094

Note : Use --batch-size 1 for latest version of Kafka

Note down all the topics from all partitions and all the servers and check in the logs


Topic	Partition	Server

First thing

This is second message

Third one

Fourth component

Fifth is coming



Try stopping 2nd server and check the consumer.

Observation 1: It works without any problem. This means Kafka is Fault tolerant.

Observation 2: Only data available on 2nd server might not be shown. This can be overcome using replication factor



Lab 11 : Multi-node Cluster with Replication Factor

Step 1: Start all the 3 kafka servers in 3 separate terminals
 

~/kafka/bin/kafka-server-start.sh ~/kafka/config/server.properties ~/kafka/bin/kafka-server-start.sh ~/kafka/config/server1.properties ~/kafka/bin/kafka-server-start.sh ~/kafka/config/server2.properties
 

Step 2: Create a topic named demo_testing2 with 7 partitions This topic will be created in multiple partitions across 3 brokers

kafka-topics.sh --create --topic demo_testing2 --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --replication-factor 3 -- partitions 7

Step 3: Check who is the leader of each partition using describe kafka-topics.sh --describe --topic demo_testing2 --bootstrap-
server localhost:9092,localhost:9093,localhost:9094

Note down the Leader and ISR ( In Sync Replicas ) for every partition

Step 4: Start a producer

kafka-console-producer.sh --topic demo_testing2 --bootstrap-server localhost:9092, localhost:9093, localhost:9094

Task : Give months as input in producer and check the consumer with and without broker 2

Step 5: Start a consumer

kafka-console-consumer.sh --topic demo_testing2 --from-beginning -- bootstrap-server localhost:9092, localhost:9093, localhost:9094

Note : Use --batch-size 1 for latest version of Kafka

Try stopping 2nd server and check the consumer.

Observation : It works without any problem. This means Kafka is Fault tolerant and data is also visible since it is replicated


WARN [Producer clientId=console-producer] Connection to node 2 (DESKTOP-8M3M600.localdomain/127.0.1.1:9094) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)



























ACL in Kafka
Add given lines to server.properties
authorizer.class.name=kafka.security.authorizer.AclAuthorizer
allow.everyone.if.no.acl.found=true

Adding ACLs – 


~/kafka/bin/kafka-acls.sh --bootstrap-server localhost:9092 --add --allow-principal User:Bob --allow-principal User:Alice --allow-host 198.51.100.0 --allow-host 198.51.100.1 --operation Read --operation Write --topic test-acl


List ACLs

~/kafka/bin/kafka-acls.sh --bootstrap-server localhost:9092 --list --topic test-acl



Reference : Apache Kafka

Kafka Connect

Lab 1 :  Basic Kafka connector to import data from a file to Kafka topic and export data from Kafka topic to a file
Configuration files (/kafka/config) required –
•	connect-file-source.properties
•	connect-file-sink.properties
•	connect-standalone.properties
Connectors are available in .jar files - /kafka/libs
For this we will require connector as, connect-file-3.4.1.jar
Step 1 : Specify path of required connector in ‘connect-standalone.properties’ using parameter, plugin.path
It should look like,
plugin.path=/home/pak/kafka/libs/connect-file-3.4.1.jar
Step 2: Create test.txt file as source file and provide its path in ‘connect-file-source.properties’ file. It should look like,
file=/home/pak/kafka/bin/test.txt
Step 3: Specify path and name of sink file, which will be automatically created in ‘connect-file-sink.properties’ file
file=/home/pak/kafka/bin/test.sink.txt
Step 4: Execute command
~/kafka/bin/connect-standalone.sh ~/kafka/config/connect-standalone.properties ~/kafka/config/connect-file-source.properties ~/kafka/config/connect-file-sink.properties

Note : topic specified in ‘connect-file-source.properties’ will be automatically created. Whenever, source file is newly created or executing a command mentioned in step 4 again after shutting down, delete file ‘/tmp/connect.offset’. This file tracks the offset so that connectors can resume from the last one. 

Lab 2: Use Kafka connect to store data into database (PostgreSQL)
Connector kafka-connect-jdbc-10.7.2.jar (kafka/libs) will be used for connection. It comes along with Kafka installation
Step 1: Install PostgreSQL if not there. And create table customer with 2 columns cid (int) and cname (string)
sudo apt install postgresql
Check PostgreSQL clusters running on system
pak@SRV1:/mnt/c/WINDOWS/system32$ pg_lsclusters

 
If cluster is down, then start one of the cluster executing given command
pak@SRV1:/mnt/c/WINDOWS/system32$ sudo pg_ctlcluster 12 main start

 
Here 12 is cluster number.
PostgreSQL prompt
pak@SRV1:/mnt/c/WINDOWS/system32$ sudo -u postgres psql

PostgreSQL commands
•	Get list of databases
postgres-# \l
•	Change database
postgres-# \c main
•	Get list of tables
main-# \dt
•	Create table
main=# create table customer (cid integer primary key, cname varchar);

Step 2 : Create new configuration file namely, ‘connect-jdbc-sink.properties’, inside folder ‘kafka/config’. Specify given configuration values in created file.
name=jdbc-sink-connector
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector
connection.url=jdbc:postgresql://localhost:5432/main
connection.user=postgres
connection.password=postgres
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true
tasks.max=1
topics=test-jdbc
#auto.create=true
auto.evolve=true
#pk.mode=none
pk.mode=record_value
pk.fields=cid
table.name.format=customer
insert.mode.databaselevel=true

Step 3: Property inside file connect-standalone.properties should look like,
plugin.path=/home/pak/kafka/libs   Done
Step 4: Execute given command for connector execution
~/kafka/bin/connect-standalone.sh ~/kafka/config/connect-standalone.properties ~/kafka/config/connect-jdbc-sink.properties

Step 5: Open new terminal and call producer with topic name specified in connect-jdbc-sink.properties file.
kafka-console-producer.sh --topic test-jdbc --bootstrap-server localhost:9092

Step 6: Give given json value as input to producer.
{"schema": 
{"type": "struct",
"fields": [  
{"type": "int32","optional": false,"field": "cid"}, 
{"type": "string","optional": false,"field": "cname"}
],
"optional": false,"name": "Person"},
"payload": {"cid": 16,"cname": "Raghu Sharma"}
}

Step 7: Check in PostgreSQL, whether value is populated in table or not.

Reference for configuration parameters for JDBC sink connector
JDBC Sink Connector Configuration Properties | Confluent Documentation

KIP-301: Schema Inferencing for JsonConverter - Apache Kafka - Apache Software Foundation


Notes : JsonConverter with schemas.enable requires "schema" and "payload" fields

















Kafka Streams

Kafka Streams require installation of Maven and Java
Command for installing Java 8

sudo apt-get install openjdk-8-jdk

Installation of Maven

wget https://dlcdn.apache.org/maven/maven-3/3.9.2/binaries/apache-maven-3.9.2-bin.tar.gz

tar -xvf apache-maven-3.9.2-bin.tar.gz

sudo mv apache-maven-3.9.2 /opt/maven

sudo nano /etc/profile.d/maven.sh

Copy given values in a file 

export JAVA_HOME=/usr/lib/jvm/default-java
export M2_HOME=/opt/maven
export MAVEN_HOME=/opt/maven
export PATH=${M2_HOME}/bin:${PATH}

Mention path for Java 8 installation for JAVA_HOME parameter

To get Java path use command and get path for Java 8

readlink -f $(which javac)

Or

whereis java

cd /usr/bin

ls -l java

ls -l /etc/alternatives/java

Or

Java 8 can be found at the location

cd /usr/lib/jvm

In most systems, path for Java 8 installation is : /usr/lib/jvm/ java-8-openjdk-amd64

sudo chmod +x /etc/profile.d/maven.sh

source /etc/profile.d/maven.sh

Check Apache Maven version

mvn –-version

It shows successful installation of Maven on system


Lab 1: Create simple Pipe.java streaming application. It will read values from one Kafka topic and load it into another Kafka topic.

Apache Kafka

Step 1: Create 2 topics as,

kafka-topics.sh --create --topic streams-plaintext-input --bootstrap-server localhost:9092

kafka-topics.sh --create --topic streams-pipe-output --bootstrap-server localhost:9092

Step 2: Execute command to setup streaming project using Kafka Streams Maven Archetype.

mvn archetype:generate     -DarchetypeGroupId=org.apache.kafka     -DarchetypeArtifactId=streams-quickstart-java     -DarchetypeVersion=3.5.0     -DgroupId=streams.examples     -DartifactId=streams.examples     -Dversion=0.1     -Dpackage=myapps

Step 3: Go to directory ‘streams.examples’. 

cd streams.examples/
Open pom.xml file

nano pom.xml

Set Kafka-version as, 3.4.1 and maven-compiler-plugin version as 3.6.1

Step 4: Delete existing examples from folder  

rm src/main/java/myapps/*.java

Step 5: Create .java file for writing our own code

cd src/main/java/myapps/

nano Pipe.java

Copy the code

package myapps;

import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.Topology;

import java.util.Properties;
import java.util.concurrent.CountDownLatch;

public class Pipe {

    public static void main(String[] args) throws Exception {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-pipe");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

        final StreamsBuilder builder = new StreamsBuilder();

        builder.stream("streams-plaintext-input").to("streams-pipe-output");

        final Topology topology = builder.build();
        final KafkaStreams streams = new KafkaStreams(topology, props);
        final CountDownLatch latch = new CountDownLatch(1);

        // attach shutdown handler to catch control-c
        Runtime.getRuntime().addShutdownHook(new Thread("streams-shutdown-hook") {
            @Override
            public void run() {
                streams.close();
                latch.countDown();
            }
        });

        try {
            streams.start();
            latch.await();
        } catch (Throwable e) {
            System.exit(1);
        }
        System.exit(0);
    }
}

Step 6: Come back to folder /streams.examples

mvn clean package
mvn exec:java -Dexec.mainClass=myapps.Pipe

It will start the streaming application

Step 7: Open another terminal to start producer.

kafka-console-producer.sh --topic streams-plaintext-input --bootstrap-server localhost:9092

Step 8: Open another terminal to start consumer

kafka-console-consumer.sh --topic streams-pipe-output --from-beginning --bootstrap-server localhost:9092

Check data sent by producer is received by consumer or not. This will happen through streaming pipeline written in Pipe.java

Maven installation
How to Install Apache Maven on Ubuntu 22.04 (linuxhint.com)


Lab 2: 

 

 

